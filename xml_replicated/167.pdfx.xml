<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>13037760f3fbf0e71b3eec83959d08178612019267b6547d09d16c4b053d1d6c</job>
    <base_name>2b</base_name>
    <doi>http://dx.doi.org/10.1111/j.1467-9280.2008.02051.x</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">PS YC HOLOGICA L SC IENCE</outsider>
      <region class="unknown" id="2">Research Report</region>
      <title-group>
        <article-title class="DoCO:Title" id="3">Head Up, Foot Down</article-title>
      </title-group>
      <region class="unknown" id="4">Object Words Orient Attention to the Objects’ Typical Location Zachary Estes, 1 Michelle Verges, 2 and Lawrence W. Barsalou 3 1 University of Warwick; 2 Indiana University, South Bend; and 3 Emory University</region>
      <abstract class="DoCO:Abstract" id="7">ABSTRACT— Many objects typically occur in particular locations, and object words encode these spatial associa- tions. We tested whether such object words (e.g., orient attention toward the location where the denoted object typically occurs (i.e., up, down). Because object words elicit perceptual simulations of the denoted objects (i.e., the representations acquired during actual perception are reactivated), we predicted that an object word would interfere with identification of an unrelated visual target subsequently presented in the object’s typical location. Consistent with this prediction, three experiments demonstrated that words denoting objects that typically occur high in the visual field hindered identification of targets appearing at the top of the display, whereas words denoting low objects hindered target identification at the bottom of the display. Thus, object words oriented attention to and activated perceptual simulations in the objects’ typical locations. These results shed new light on how language affects perception.</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="5" page="1" column="1">head,</h1>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="6" page="1" column="1">foot)</h1>
      </section>
      <region class="DoCO:TextChunk" id="13" page="1" column="1">Attention is often guided by environmental cues (see Berger, Henik, &amp; <xref ref-type="bibr" rid="R2" id="8" class="deo:Reference">Rafal, 2005</xref>). Here we focus on cues that orient attention away from themselves. For example, when preceded by an arrow pointing leftward (Posner, Snyder, &amp; <xref ref-type="bibr" rid="R12" id="9" class="deo:Reference">Davidson, 1980</xref>), the word left (Hommel, Pratt, Colzato, &amp; <xref ref-type="bibr" rid="R5" id="10" class="deo:Reference">Godijn, 2001</xref>), a head facing leftward (Langton, Watt, &amp; <xref ref-type="bibr" rid="R9" id="11" class="deo:Reference">Bruce, 2000</xref>), or eyes gazing leftward (Kingstone, Smilek, Ristic, Friesen, &amp; <xref ref-type="bibr" rid="R7" id="12" class="deo:Reference">Eastwood, 2003</xref>), visual targets are identified faster on the left than on the right. These directional cues orient attention even when the target is no more likely to occur at the cued location than at an uncued location. Thus, social and symbolic cues can reflexively orient attention to an implied location.</region>
      <region class="DoCO:TextChunk" id="15" confidence="possible" page="1" column="1">Address correspondence to Zachary Estes, Department of Psychology, University of Warwick, Coventry, CV4 7AL, United Kingdom, e-mail: <email id="14">z.estes@warwick.ac.uk</email>.</region>
      <region class="DoCO:TextChunk" id="28" page="1" column="2">Do object words such as head and foot similarly direct attention to specific locations? Words that denote objects are, after all, both symbolic and social. They are verbal symbols for in- terpersonal communication. Some objects, such as apples and books, occur in diverse locations and hence have no particular spatial connotation. Other objects, however, typically occur in particular locations. For instance, branches and clouds are typically overhead, whereas roots and puddles are typically underfoot. Indeed, object words are judged faster when presented on a computer screen in the objects’ canonical locations ( <xref ref-type="bibr" rid="R20" id="16" class="deo:Reference">Zwaan &amp; Yaxley, 2003</xref>). For instance, eagle is judged faster when presented at the top rather than the bottom of a display, whereas snake is judged faster at the bottom (S ˇ eti  ́ &amp; <xref ref-type="bibr" rid="R16" id="17" class="deo:Reference">Domijan, 2007</xref>). Given that many object words encode spatial associa- tions, we hypothesized that an object word directs attention toward the location where its referent typically occurs. Whereas directional cues such as left simply point to a location, object words such as bird also evoke a perceptual simulation of the denoted object. Perceptual simulation is the activation of perceptual representations that were acquired during the actual perception of a stimulus. More specifically, it is the reactivation of neural pathways that have become associated with perceiving a particular stimulus (see Barsalou, 1999, in press; <xref ref-type="bibr" rid="R10" id="18" class="deo:Reference">Martin, 2007</xref>). For example, the word bird and an image of a bird activate highly overlapping cortical networks (Vandenberghe, Price, Wise, Josephs, &amp; <xref ref-type="bibr" rid="R18" id="19" class="deo:Reference">Frackowiak, 1996</xref>; see Pulvermu  ̈ ller, 2001). Such perceptual simulations emerge au- tomatically during language comprehension, often without conscious awareness. They may also be intentionally generated and consciously inspected, as in the case of mental imagery. Perceptual simulation of word meaning may have significant implications for attention and perception. If the word bird activates the neural mechanisms involved in the perception of a bird, then perception of another visual stimulus that requires these same mechanisms should be delayed. Indeed, this may explain why mental imagery hinders visual perception (Craver- <xref ref-type="bibr" rid="R4" id="20" class="deo:Reference">Lemley &amp; Reeves, 1992</xref>), particularly when the mental image<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> and the physical stimulus overlap spatially (Craver-<xref ref-type="bibr" rid="R3" id="26" class="deo:Reference">Lemley &amp; Arterberry, 2001</xref>). Generalizing to more implicit forms of perceptual simulation, we hypothesized that object words might also hinder visual perception. More specifically, if an object word activates a perceptual simulation in the object’s typical location, then that word should hinder perception of a visual target in that location. Essentially, because the perceptual mechanisms required for target identification are engaged in the simulation of the denoted object at its typical location, identification of a target at that location should be delayed. In contrast, when the visual target and the perceptual simulation do not overlap spatially, interference should not occur (cf. Craver- <xref ref-type="bibr" rid="R3" id="27" class="deo:Reference">Lemley &amp; Arterberry, 2001</xref>). The experiments reported here assessed this hypothesis.</region>
      <outsider class="DoCO:TextBox" type="footer" id="22" page="1" column="2">Volume 19—Number 2</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="23" page="1" column="2">Copyright r 2008 Association for Psychological Science</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="24" page="1" column="2">93</outsider>
      <outsider class="DoCO:TextBox" type="header" id="25" page="2" column="1">Head Up, Foot Down</outsider>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="29" confidence="possible" page="2" column="1">EXPERIMENT 1</h1>
        <region class="DoCO:TextChunk" id="30" page="2" column="1">In this experiment, object words associated with an upper or lower location served as cues. To ensure that the object words unambiguously denoted an upper or lower location, we presented a context word before each object word. Thus, on each trial, a context word (e.g., cowboy) was followed by an upper- location (e.g., hat) or lower-location (e.g., boot) cue. Context and cue words were both presented centrally. A target letter (X or O) then appeared at the top or bottom of the display, and participants identified the target as quickly as possible. The target was equally likely to appear at the top or bottom location, regardless of the cue object’s typical location. Thus, the cue did not predict the target’s location. If an object word activates a perceptual simulation of the denoted object in its typical location, then target identification should be hindered at that location.</region>
      </section>
      <section class="deo:Methods">
        <h1 class="DoCO:SectionTitle" id="31" page="2" column="1">Method</h1>
        <region class="DoCO:TextChunk" id="32" page="2" column="1">The experimental stimuli were 30 context words, each paired with one upper-location and one lower-location cue word (i.e., 60 spatial cues). We also included 60 filler stimuli consisting of 30 context words, each paired with 2 nonspatial cue words (e.g., chocolate powder, chocolate shavings ). Participants initiated each trial by pressing the space bar, which triggered a central fixation cross that appeared for 250 ms. The context word then appeared centrally for 500 ms, replaced immediately by the cue word for 250 ms. After a 50-ms delay, a target letter subtending approximately 11 of visual angle appeared at the top or bottom of the screen. The ‘‘top’’ and ‘‘bottom’’ locations were centered horizontally approximately 81 vertically from the center of the display. Participants were instructed to identify each target letter as quickly and accurately as possible by pressing the appropriate key. Location cue (upper, lower), target location (top, bottom), and target letter (X, O) were fully crossed and balanced, such that each target letter was equiprobable at each target location, and each target location was equiprobable within each cue condition. Ten practice trials preceded the 120 experimental trials. Eighteen undergraduates participated for course credit.</region>
      </section>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="33" page="2" column="2">Results and Discussion</h1>
        <region class="DoCO:TextChunk" id="35" page="2" column="2">Data were coded according to whether the target letter appeared in the location associated with the object word. The typical condition included upper-location cues followed by top targets and lower-location cues followed by bottom targets, whereas the atypical condition included upper-location cues followed by bottom targets and lower-location cues followed by top targets. Response times on incorrect trials were removed from all analyses. Response times greater than 1,000 ms were also removed, resulting in the exclusion of 1% to 4% of trials (across experiments). Data were analyzed using analysis of variance (ANOVA) across participants (F 1 ) and items (F 2 ). Targets were identified more slowly and less accurately when they appeared in the typical location of the object denoted by the preceding word than when they appeared in the opposite location (see <xref ref-type="fig" rid="F1" id="34" class="deo:Reference">Fig. 1</xref>, left-most graphs). In the response time data, this effect was significant, F 1 (1, 17) 5 40.19, p rep % 1.00, and F 2 (1, 58) 5 41.54, p rep % 1.00, and was robust (37 ms, Z 2 5 .70). In the error-rate data, this effect was also significant, F 1 (1, 17) 5 9.33, p rep 5 .97, and F 2 (1, 58) 5 18.96, p rep % 1.00, and robust (4.83%, Z 2 5 .35). As predicted, the cue word evoked a perceptual simulation in the object’s typical location, thus hindering perception of a target letter at that location.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="36" confidence="possible" page="2" column="2">EXPERIMENT 2</h1>
        <region class="DoCO:TextChunk" id="39" page="2" column="2">If the interference observed in Experiment 1 was due to perceptual simulation, then disrupting that simulation should attenuate the effect. In Experiment 2, we tested this prediction by replicating the procedure of Experiment 1, but including a condition in which both target locations were visually masked prior to presentation of the target. Another possibility is that the observed effect reflected inhibition of return (IOR), whereby the perception of a stimulus at a recently attended location is tem- porarily inhibited (<xref ref-type="bibr" rid="R11" id="37" class="deo:Reference">Posner &amp; Cohen, 1984</xref>). The spatial cue may have elicited an attentional shift to the implied location, thereby triggering IOR and inhibiting target identification in that location. Indeed, in Experiment 1, the target appeared 300 ms after the spatial cue, a delay that is well within the typical onset of IOR around 225 ms poststimulus (see <xref ref-type="bibr" rid="R8" id="38" class="deo:Reference">Klein, 2000</xref>). To test this explanation, we reduced the cue-target asynchrony to 150 ms in Experiment 2. Any interference at this brief delay could not be attributed to IOR.</region>
      </section>
      <section class="deo:Methods">
        <h1 class="DoCO:SectionTitle" id="40" page="2" column="2">Method</h1>
        <region class="DoCO:TextChunk" id="45" page="2" column="2">Fifty-nine undergraduates were randomly assigned to one of two conditions: unmasked or masked. In the unmasked condition, the stimuli and procedure were the same as in Experiment 1, except that the context and cue words appeared for only 150 and 100 ms, respectively. With the 50-ms delay preceding the target, the cue-target asynchrony was 150 ms. The masked condition was identical, except that a visual mask appeared during the 50- ms delay. The mask—three contiguous rows of eight ampersands <marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> (approximately 31 Â 5.251)—appeared simultaneously at the two target locations on each trial.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="42" page="2" column="2">94</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="43" page="2" column="2">Volume 19—Number 2</outsider>
        <outsider class="DoCO:TextBox" type="header" id="44" page="3" column="1">Zachary Estes, Michelle Verges, and Lawrence W. Barsalou</outsider>
      </section>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="46" page="3" column="1">Results and Discussion</h1>
        <region class="DoCO:TextChunk" id="48" page="3" column="1">Data were analyzed as in Experiment 1. Six participants (3 from each group) were excluded because of overall latencies or ac- curacies more than 2.5 standard deviations from the group mean. Data were initially analyzed via 2 (target location: typical, atypical) Â 2 (mask: unmasked, masked) mixed ANOVAs. As <xref ref-type="fig" rid="F1" id="47" class="deo:Reference">Figure 1</xref> illustrates, the perceptual interference effect on both response times and error rates was replicated in the unmasked condition. In the masked condition, however, these effects were attenuated. In the response time data, the main effect of target location was significant, F 1 (1, 51) 5 18.28, p rep % 1.00, and F 2 (1, 58) 5 155.18, p rep % 1.00; target identification was again slower in the typical location of the preceding object word. The main effect of mask was also significant, F 1 (1, 51) 5 16.54, p rep 5 .99, and F 2 (1, 58) 5 509.41, p rep % 1.00; visual masking slowed target identification. Most important, however, target</region>
        <region class="unknown" id="49" page="3" column="1">Typical</region>
        <disp-formula class="DoCO:FormulaBox" id="Fms">
          <label class="DoCO:Label" id="50">ms</label>
          <content class="DoCO:Formula" id="51" page="3" column="1">580 560 a *** c 540</content>
        </disp-formula>
        <disp-formula class="DoCO:FormulaBox" id="Fms">
          <label class="DoCO:Label" id="52">ms</label>
          <content class="DoCO:Formula" id="53" page="3" column="1">*** 520</content>
        </disp-formula>
        <region class="unknown" id="54" page="3" column="1">Time 500 480 Response 440 460 420 400 380 10 9 b d 8 ** (%) 7 * 6 Rate 5 Error 4 3 2 1 0 Experiment 1 Experiment 2, Unmasked</region>
        <region class="DoCO:FigureBox" id="F1">
          <caption class="deo:Caption" id="55" page="3" column="1">Fig. 1. Mean response times (top row) and error rates (bottom row) as a function of target location (typical, atypical). From left to right, the graphs present results for Experiment 1, the unmasked condition of Experiment 2, the masked condition of Experiment 2, and Experiment 3. Error bars show standard errors. Asterisks indicate a significant difference between results for typical and atypical locations, n p &lt; .05, nn p &lt; .01, nnn p &lt; .001.</caption>
        </region>
        <region class="DoCO:TextChunk" id="67" page="3" column="2">location and mask interacted: The interference effect was larger in the unmasked condition than in the masked condition, F 1 (1, 51) 5 4.31, p rep 5 .89, and F 2 (1, 58) 5 32.70, p rep % 1.00. In the error-rate data, only the main effect of target location was significant, F 1 (1, 51) 5 8.01, p rep 5 .96, and F 2 (1, 58) 5 13.21, p rep 5 .99. The unmasked and masked conditions were also analyzed separately, and we discuss the results of those analyses next. <marker type="block"/> Unmasked Condition In the response time data for the unmasked condition, perceptual interference was significant, F 1 (1, 25) 5 19.65, p rep 5 .99, and F 2 (1, 58) 5 176.11, p rep % 1.00, and was robust (74 ms, Z 2 5 .44). In the error-rate data, interference was also significant, F 1 (1, 25) 5 7.24, p rep 5 .94, and F 2 (1, 58) 5 11.49, p rep 5 .98, and robust (1.60%, Z 2 5 .23).<marker type="block"/> Masked Condition Response times in the masked condition yielded only mixed evidence of perceptual interference, F 1 (1, 26) 5 2.48, p rep 5<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> .79, and F 2 (1, 58) 5 27.90, p rep % 1.00. The error-rate data for the masked condition did not show significant interference.<marker type="block"/> Summary The perceptual interference observed in Experiment 1 was replicated in the unmasked condition, but was attenuated in the masked condition. Although the same qualitative pattern was observed in the two conditions, the significant interactive effect of target location and mask on response times indicates that these patterns differed quantitatively. Furthermore, error rates showed the interference effect in the unmasked condition, but not in the masked condition. Given the brief cue-target asynchrony (150 ms), perceptual interference cannot be explained as IOR, which occurs only at longer delays (<xref ref-type="bibr" rid="R8" id="66" class="deo:Reference">Klein, 2000</xref>). Thus, an object word appears to elicit a perceptual simulation in the object’s typical location, and this perceptual simulation interferes with perceiving a target in that location. A visual mask in that location, however, disrupts the simulation and hence attenuates its interfering effect.</region>
        <region class="unknown" id="59" page="3" column="2">Atypical e g ***</region>
        <region class="unknown" id="60" page="3" column="2">f h ***</region>
        <region class="unknown" id="61" page="3" column="2">Experiment 2, Experiment 3 Masked</region>
        <outsider class="DoCO:TextBox" type="footer" id="62" page="3" column="2">Volume 19—Number 2</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="63" page="3" column="2">95</outsider>
        <outsider class="DoCO:TextBox" type="header" id="64" page="4" column="1">Head Up, Foot Down</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="68" confidence="possible" page="4" column="1">EXPERIMENT 3</h1>
        <region class="DoCO:TextChunk" id="69" page="4" column="1">In the preceding experiments, the location cue (e.g., hat) was preceded by a context word (e.g., cowboy). To ensure that interference was unrelated to the context words, we presented only the location cues in Experiment 3.</region>
      </section>
      <section class="deo:Methods">
        <h1 class="DoCO:SectionTitle" id="70" page="4" column="1">Method</h1>
        <region class="DoCO:TextChunk" id="71" page="4" column="1">The stimuli and procedure were identical to those in the unmasked condition of Experiment 2, except that only cue words were presented. Thirty undergraduates participated.</region>
      </section>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="72" page="4" column="1">Results and Discussion</h1>
        <region class="DoCO:TextChunk" id="75" page="4" column="1">Data were analyzed as in Experiment 1. Three participants with outlying data (2.5 standard deviations beyond the mean) were excluded. The response times (see <xref ref-type="fig" rid="F1" id="73" class="deo:Reference">Fig. 1</xref>, upper right graph) showed an interference effect that was significant, F 1 (1, 26) 5 22.59, p rep % 1.00, and F 2 (1, 58) 5 25.15, p rep % 1.00, and robust (32 ms, Z 2 5 .47). The error rates (see <xref ref-type="fig" rid="F1" id="74" class="deo:Reference">Fig. 1</xref>, lower right graph) also showed an interference effect that was significant, F 1 (1, 26) 5 12.77, p rep 5 .98, and F 2 (1, 58) 5 13.04, p rep 5 .99, and robust (3.21%, Z 2 5 .33). Thus, an object word orients attention to and evokes a perceptual simulation in the denoted object’s typical location, thereby hindering perception in that location.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="76" confidence="possible" page="4" column="1">GENERAL DISCUSSION</h1>
        <region class="DoCO:TextChunk" id="92" page="4" column="1">Words denoting objects that typically occur in high places (e.g., hat, cloud) hindered identification of targets appearing at the top of the display, whereas words denoting objects that typically occur in low places (e.g., boot, puddle) hindered identification of targets at the bottom. This perceptual interference is attribut- <marker type="column" number="2"/><marker type="block"/> able to attentional orienting and perceptual simulation. First, an object word orients attention toward the object’s typical location. For example, because birds typically are seen in high places, they become associated with the upper visual field (cf. Richardson &amp; <xref ref-type="bibr" rid="R14" id="78" class="deo:Reference">Spivey, 2000</xref>), and the word bird elicits an upward shift of attention. Second, the object word activates a perceptual simulation of the denoted object (Barsalou, in press; <xref ref-type="bibr" rid="R10" id="79" class="deo:Reference">Martin, 2007</xref>). That is, the neural systems associated with the actual perception of the denoted object are activated. In the experiments reported here, object words were followed by a visual target at either the top or the bottom of a display, such that the target appeared in either the denoted object’s typical location or the opposite location. Because the simulated object (e.g., a bird) and the perceptual target (e.g., X) shared few features, perception of the target required inhibition of the neural circuits activated during object simulation, thereby slowing target identification at the denoted object’s typical location. At the opposite (atypical) location, the perceptual mechanisms were not engaged in object simulation, and hence target identification proceeded without interference. Attentional orienting and perceptual simulation together explain why some linguistic cues hinder perception of a physical object whereas others facilitate perception of the object. When the linguistic cue activates a perceptual representation that shares few features with the physical stimulus, as in the present experiments, perception is delayed. <xref ref-type="bibr" rid="R15" id="80" class="deo:Reference">Richardson, Spivey, Barsalou, and McRae (2003)</xref> presented sentences describing either a vertically oriented event (e.g., X respected Y) or a horizontally oriented event (e.g., X argued with Y), followed by an unrelated visual target (i.e., a square or circle) at the top, bottom, left, or right of the display. Targets on the axis associated with the preceding sentence (e.g., a vertical event followed by a top or bottom target) were identified more slowly than targets associated with the other axis (e.g., a vertical event followed by a left or right target). <xref ref-type="bibr" rid="R6" id="81" class="deo:Reference">Kaschak et al. (2005)</xref> reported an analogous pattern of interference between visual depictions of motion and unrelated sentential descriptions of motion. The present results suggest that participants in all these studies attended to the location, axis, or direction implied by the linguistic cue and simulated the described object or activity. Interference occurred because the simulation and the visual stimulus activated dif- ferent perceptual representations. In contrast, when the linguistic cue activates many perceptual features of the physical stimulus, perception is facilitated. <xref ref-type="bibr" rid="R17" id="82" class="deo:Reference">Stanfield and Zwaan (2001)</xref> presented prime sentences that described an object in one orientation or another (e.g., ‘‘He hammered the nail into the wall/floor’’), and each sentence was followed by an image of the target object in one of these orientations (e.g., a nail oriented horizontally or vertically). Judg- ments of the target object were faster when it matched the orientation implied by the sentence. Similarly, <xref ref-type="bibr" rid="R19" id="83" class="deo:Reference">Zwaan, Madden, Yaxley, and Aveyard (2004)</xref> found that a ball in motion was processed faster when preceded by a sentence describing a ball<marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> moving in the same direction rather than the opposite direction. In both cases, facilitation occurred because the simulation and the visual stimulus activated similar perceptual representations. These results have implications not only for perception, but also for attention. When a central cue orients attention to a peripheral location, despite being nonpredictive of the actual location of the target, that orienting is said to be reflexive, or automatic. For example, symbolic cues such as arrows, head orientation, and eye-gaze direction elicit reflexive orienting (<xref ref-type="bibr" rid="R5" id="88" class="deo:Reference">Hommel et al., 2001</xref>; <xref ref-type="bibr" rid="R7" id="89" class="deo:Reference">Kingstone et al., 2003</xref>; <xref ref-type="bibr" rid="R9" id="90" class="deo:Reference">Langton et al., 2000</xref>; <xref ref-type="bibr" rid="R12" id="91" class="deo:Reference">Posner et al., 1980</xref>). Notably, because the object words in the present experiments did not predict the location of the target, our results indicate that orienting from object words is reflexive. And given the ubiquity of object words, these results indicate that reflexive orienting from symbolic cues is more general than previously known.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="85" page="4" column="2">96</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="86" page="4" column="2">Volume 19—Number 2</outsider>
        <outsider class="DoCO:TextBox" type="header" id="87" page="5" column="1">Zachary Estes, Michelle Verges, and Lawrence W. Barsalou</outsider>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="93" confidence="possible" page="5" column="1">REFERENCES</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="94" page="5" column="1">Barsalou, L.W. (1999). Perceptual symbol systems. Behavioral and Brain Sciences, 22, 577–660.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="95" confidence="possible" page="5" column="1">Barsalou, L.W. (in press). Grounded cognition. Annual Review of Psychology. Berger, A., Henik, A., &amp; Rafal, R. (2005). Competition between en- dogenous and exogenous orienting of visual attention. Journal of Experimental Psychology: General, 134, 207–221.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="96" confidence="possible" page="5" column="1">Craver-Lemley, C., &amp; Arterberry, M.E. (2001). Imagery-induced interference on a visual detection task. Spatial Vision, 14, 101–119.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="97" confidence="possible" page="5" column="1">Craver-Lemley, C., &amp; Reeves, A. (1992). How visual imagery interferes with vision. Psychological Review, 99, 633–649.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="98" confidence="possible" page="5" column="1">Hommel, B., Pratt, J., Colzato, L., &amp; Godijn, R. (2001). Symbolic control of visual attention. Psychological Science, 12, 360–365.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="99" confidence="possible" page="5" column="1">Kaschak, M.P., Madden, C.J., Therriault, D.J., Yaxley, R.H., Aveyard, M., Blanchard, A.A., &amp; Zwaan, R.A. (2005). Perception of motion affects language processing. Cognition, 94, B79–B89.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="100" confidence="possible" page="5" column="1">Kingstone, A., Smilek, D., Ristic, J., Friesen, C.K., &amp; Eastwood, J.D. (2003). Attention, researchers! It is time to take a look at the real world. Current Directions in Psychological Science, 12, 176–180.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="101" page="5" column="2">Klein, R.M. (2000). Inhibition of return. Trends in Cognitive Sciences, 4, 138–147.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="102" confidence="possible" page="5" column="2">Langton, S.R.H., Watt, R.J., &amp; Bruce, V. (2000). Do the eyes have it? Cues to the direction of social attention. Trends in Cognitive Sciences, 4, 50–59.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="103" confidence="possible" page="5" column="2">Martin, A. (2007). The representation of object concepts in the brain. Annual Review of Psychology, 58, 25–45.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="104" confidence="possible" page="5" column="2">Posner, M.I., &amp; Cohen, Y. (1984). Components of visual orienting. In H. Bouma &amp; D.G. Bouwhuis (Eds.), Attention and performance X: Control of language processes (pp. 531–556). Hillsdale, NJ: Erlbaum.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="105" confidence="possible" page="5" column="2">Posner, M.I., Snyder, C.R.R., &amp; Davidson, B. (1980). Attention and the detection of signals. Journal of Experimental Psychology: General, 109, 160–174.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="106" confidence="possible" page="5" column="2">Pulvermu  ̈ ller, F. (2001). Brain reflections of words and their meaning. Trends in Cognitive Sciences, 5, 517–524.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="107" confidence="possible" page="5" column="2">Richardson, D.C., &amp; Spivey, M.J. (2000). Representation, space, and Hollywood Squares: Looking at things that aren’t there anymore. Cognition, 76, 269–295.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="108" confidence="possible" page="5" column="2">Richardson, D.C., Spivey, M.J., Barsalou, L.W., &amp; McRae, K. (2003). Spatial representations activated during real-time comprehension of verbs. Cognitive Science, 27, 767–780.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="109" confidence="possible" page="5" column="2">S ˇ eti  ́, M., &amp; Domijan, D. (2007). The influence of vertical spatial orientation on property verification. Language and Cognitive Processes, 22, 297–312.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="110" confidence="possible" page="5" column="2">Stanfield, R.A., &amp; Zwaan, R.A. (2001). The effect of implied orientation derived from verbal context on picture recognition. Psychological Science, 12, 153–156.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="111" confidence="possible" page="5" column="2">Vandenberghe, R., Price, C., Wise, R., Josephs, O., &amp; Frackowiak, R.S.J. (1996). Functional anatomy of a common semantic system for words and pictures. Nature, 383, 254–256.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="112" confidence="possible" page="5" column="2">Zwaan, R.A., Madden, C.J., Yaxley, R.H., &amp; Aveyard, M.E. (2004). Moving words: Dynamic representations in language comprehension. Cognitive Science, 28, 611–619.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="113" confidence="possible" page="5" column="2">Zwaan, R.A., &amp; Yaxley, R.H. (2003). Spatial iconicity affects semantic relatedness judgments. Psychonomic Bulletin &amp; Review, 10, 954–958.</ref>
          <ref class="deo:BibliographicReference" id="114" page="5" column="2">(R ECEIVED 5/2/07; R EVISION ACCEPTED 8/2/07)</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="footer" id="115" page="5" column="2">Volume 19—Number 2</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="116" page="5" column="2">97</outsider>
      </section>
    </body>
  </article>
</pdfx>
