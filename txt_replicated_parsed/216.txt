 There is no doubt that many of our most fundamental abilities, whether they concern language, perception, motor skill, or social behavior, reflect some kind of adaptation to the regularities of the world that evolves without intention to learn, and without a clear awareness of what we know. This ubiquitous phenomenon was called ‘implicit learning’ (IL) by Reber [ 1, 2] 40 years ago. Since then, several studies have explored this form of learning with several experimental paradigms (mainly finite-state grammars and serial reaction time tasks; for reviews, see [3, 4]). Originating from a different research tradition, the term ‘statistical learning’ (SL) was proposed 10 years ago by Saffran and collaborators [4] to designate the ability of infants to discover the words embedded in a continuous artificial language, and this field of research is now growing exponentially. There are obvious similarities between SL and IL. As in IL, participants in SL experiments are faced with structured material without being instructed to learn. They learn merely from exposure to positive instances, without engaging in analytical processes or hypothesis-testing strategies. Researchers have pointed out that SL proceeds automatically [5, 6, 7, 8], incidentally [9], spontaneously [6], or by simple observation [9], and that participants in SL settings were unaware of the statistical structure of the material [7]. This article first describes how recent evolution in IL and SL research fields has made them closer to one another, leading to a growing number of cross-references and to the occasional use of the two expressions as synonymous. Conway and Christiansen [10] even now propose the term ‘implicit statistical learning’ to cover the two domains. However, we then go on to show that beyond the similarity of paradigms and results, the two domains emphasize different interpretations of the data. We suggest that this divergence, which has not been highlighted as yet, opens up a deep challenge for future studies. Ten years ago, it seemed possible to contrast IL and SL on their main issues of interest, namely syntax acquisition and lexicon formation, respectively. Indeed, the to-be- learned material used in artificial grammar learning research is typically governed by rules, that is by organizing principles which are independent of the specific material used in a given instance. If participants learned the rules, then this form of learning would be out of the scope of SL studies, in which the notion of rules is a priori irrelevant. However, research from the past few years has made it increasingly clear that participants in artificial grammar learning experiments do not need to extract the rules to perform well, even in situations involving transfer across surface forms (Box 1). In addition, the artificial grammar learning paradigms tend to be now supplanted by other paradigms, such as the serial reaction-time tasks, in which a description of the materials in terms of rules appears less appropriate. Another initial difference between the two domains was that IL research used a large variety of situations involving different sensory modalities and response systems, whereas SL originally focused on the early stage of language acquisition. However, more recently research on SL has progressively broadened its scope of investigation. The syllables used in the first studies have been replaced by tones with the same results [11, 12]. A parallel literature has evolved with visual shapes [6, 7, 8], or even tactile stimuli [13]. Perhaps even more importantly, recent SL studies are no longer limited to the segmentation of a continuous display into word-like units, but they also explore other, more complex structures [14]. For instance, Saffran and Wilson [15] have used a finite-state grammar to generate their artificial language, and Hunt and Aslin [16] used a serial reaction time task, hence borrowing the prototypical situations of IL to investigate the properties of SL. A recent set of results on the role of attention further strengthens the similarity between IL and SL. Although a few earlier IL studies claimed that at least some forms of learning do not require attention, the bulk of recent evidence supports the opposite conclusion. For instance, Shanks and collaborators (e.g. [18]) showed that performances in serial reaction time tasks are degraded under double-task conditions (see also [19]). Likewise, Chun and Jiang (e.g. [20]) showed that implicit learning in the contextual cuing paradigm is robust only when relevant, predictive information is selectively attended to (see also [21]). In covariation learning, Hoffman and Sebald [22] showed that no learning occurs without attention, even when the to-be-learned covariations are highly salient (for reviews on earlier studies, see [23, 24]. The same conclusion emerges from studies in SL. When the performance of participants in a dual task setting is compared with that of participants attending to the to-be-learned materials, the former is always degraded compared to the latter. This has been observed in standard word segmentation tasks [25] as well as in paradigms using visual shapes [8, 26]. Thus arguably, IL and SL studies now pursue essentially the same objective - namely, the study of domain-general learning mechanisms acting on attended information in incidental, unsupervised learning situations (e.g. [17]). Although the similarities between IL and SL are impressive, comparing the interpretations favored in both fields leads us to a thought-provoking observation. In the IL literature, several models have been developed as alternatives to the initial rule-based view. The first alternative idea in artificial grammar learning research was that participants memorized the displayed strings of letters, then performed their grammaticality judgments on the basis of the similarity between the test items and the study items. The role of similarity in grammaticality judgments has been shown in some studies (e.g. [27]). However, there is also significant evidence that participants memorize fragments of strings, and that grammaticality judgments rely, at least partly, on this form of knowledge. It has been argued that the fragments or chunks provide a most efficient coding of the information, because learning makes their selection increasingly adapted to the structure of the material (Box 2). This kind of interpretation has been applied as well to other IL paradigms such as serial reaction-time tasks [28] By contrast, the interpretation proposed in the SL approach postulates that participants perform statistical computations. Evidence for segmentation is generally attributed to the ability of participants to compute some kind of conditional probabilities between successive or contiguous elements. This interpretation prevails for auditory artificial languages (e.g. [14, 15]) as well as for visual scenes (e.g. [7, 9]). At the computational level, this interpretation is generally implemented by connectionist networks, most often SRNs (e.g. [29]). Note that the contrast we draw here is not as clear-cut as our presentation suggests. There have been a few attempts to account for word segmentation with chunking models (e.g. [30]), although they have been virtually ignored in SL literature. More significantly, the performance in IL paradigms has been often simulated with SRNs (e.g. [31, 32 33]). Because SRNs, like any connectionist network, are sensitive to statistical regularities, this means that certain IL researchers have construed implicit learning as statistical computations [3, 32, 34]. However, the coexis- tence of chunk-based theories and connectionist models within the IL literature has not drawn much attention, largely because their common opposition to rule-based models overshadowed their differences. The joint consideration of IL and SL studies now brings the contrast between the two accounts on the front of the scene. Nobody denies the existence of chunk knowledge. The advocates of statistical approaches claim themselves that learning shapes some kind of psychological units. For instance, Saffran and collaborators [15, 35] have shown that training with unsegmented speech results in the formation of word-like units, rather than in strings of sounds the probability of which varies on a continuous dimension. Likewise, Baker and collaborators [26] and Fiser and Aslin [9] emphasize that the end result of SL with visual displays is the formation of objects. This leaves three possibilities to account for the available evidence: The first possibility is that statistical computations and chunk formation are independent processes. Meulemans and Van Der Linden [ 36] have argued for this view, with the additional assumption that chunk formation is responsible for conscious knowledge, and statistical computation for improved performance in implicit tasks (for related hypotheses, see [37, 38]). This hypothesis is grounded on the dissociation between performance and explicit knowledge observed in amnesic patients. However, alternative interpretations have been proposed for this dissociation, notably by Shanks and collaborators [33, 39, 40]. Shanks and collaborators assume a single knowledge basis, and hence, in addition to the advantage of better parsimony, their interpretation provides a natural account for the ubiquitous relationships observed in normal participants between conscious knowledge and performance. The second possibility is that statistical computations and chunk formation are two successive steps in the learning process. Chunks would be inferred from prior statistical computations. Typically, chunk boundaries are defined as the points where the predictibility of successive or spatially contiguous elements is the lowest. This interpretation is largely prevalent in SL research, both for oral stimuli (e.g. [35]) and for visual scenes (e.g. [9]). The third possibility is that the formation of chunks is the only effective process, with the sensitivity to statistical structure being a by-product of this process. At least two computationally implemented models illustrate this option, the Competitive Chunking model [41] and PARSER [30]. In PARSER , for instance, the chunks are formed from the outset on a random basis, as a natural consequence of the capacity-limited attentional processing of the incoming information. These chunks are then forgotten or strengthened according to the laws governing associative memory. We will now focus on the last two possibilities, the first in which chunking is based on prior statistical analyses, and the second in which chunking is a primitive process the result of which amounts to simulating statistical computations. We are not aware of empirical arguments from proponents of chunk-based theories against the models assuming statistical computations, except that this assumption could be unnecessary. By contrast, SL researchers have occasionally argued that chunk models are only sensitive to the raw frequency of co-occurrences [ 14], whereas studies in SL have shown that participants were sensitive to more subtle statistics, such as conditional (or transi- tional) probabilities (e.g. [14, 42]). Indeed, most chunk- based learning models, such as the competitive chunking model [41] or the measures of chunk strength used in the influential studies by Knowlton and collaborators (e.g. [43]) exclusively exploit frequency information, certainly because this initially appeared to be sufficient to account for a large part of the available evidence. However, the exclusive focus on frequency of many chunk-based models is somewhat surprising in itself. Although chunk-based models are claimed to implement associative learning principles, assuming that chunk memory only depends on their frequency amounts to neglecting some of the most basic laws governing the formation of associations. Indeed, it has long been known that the strength of memory traces does not only depend on the number of repetitions of the study pairs. In particular, forgetting is due in large part to the interference generated by the prior or subsequent events that are related in some way to the target event. The sequential material used in both IL and SL studies is certainly prone to generate strong and pervasive interference, because it is typically generated by recombining a small number of primitives. Now, and this is the crucial point, taking into account the effect of interference in evaluating chunk strength amounts to considering other measures of association than the raw frequency of co-occurrences. Box 3 illustrates how implementing forward interference is sufficient to make chunk strength sensitive to transi- tional probabilities, which SL researchers consider so important. Moreover, Perruchet and Peereman [44] have shown that PARSER , thanks to the role ascribed to interference in chunk formation, was even sensitive to contingency, that is to a measure of association more comprehensive than conditional probabilities. The above remarks suggest that it might turn out to be difficult to decide between concurrent interpretations based on a simple consideration of their explanatory power. Because IL and SL have mainly evolved as separate fields of research, the challenge has not often been addressed. A few recent studies, however, have begun to explore situations designed in such a way that predictions drawn from chunk-based models and statistical approaches differ. In these studies, some version of an SRN is used to quantify the predictions of statistical approaches, whereas the chunking models are rep- resented by the Competitive Chunking Model [45] or PARSER [37, 44, 46]. Although a detailed review of these studies is beyond the scope of this review, suffice it to say that, overall, their results do not clearly favor one or the other account. These preliminary results suggest that the present accounts will need to be amended. Further models would also allow to encompass data that neither the chunk- based models nor the statistical approaches in their current instantiations seem to be able to explain. In the past few years, several studies have shown the possibility of incidentally learning the relations between elements that are not contiguous in space and/or time (Box 4). Because the chunking process is usually construed as the clustering of adjacent events, these data confront the chunking models with a difficult challenge, as noted by Kuhn and Dienes [47]. In principle, they do not raise the same problem for statistical approaches, because the notion of statistical computations does not care about the nature of the data (e.g. contiguous or not) on which statistics may be computed. However, there is a consensus among researchers working on language and visual perception that models relying on statistical computations alone need to be constrained to avoid combinatorial explosion. The adjacency of the to-be-learned elements provides such a natural constraint (which is implemented, for instance, in SRNs). Thus, the possibility of learning nonadjacent dependencies entails either an in-depth revi- sion of chunk-based models, or a significant departure from the most frequent computational implementation of statistical approaches. One of the major implication of the debate outlined above is the function of consciousness in the learning process. If the chunks are inferred from the results of statistical computations, then most of the learning process must be thought of as unconscious, because statistical computations are not performed consciously in the context of incidental learning paradigms. Of course, this does not mean that chunks, once formed, are functionally inert in further steps of conscious activities, but simply that their initial emergence is guided by unconscious computations. On the other hand, if the final chunks evolve from the progressive modification of primitive chunks, then the function of consciousness in chunk formation can be construed differently. Starting from the postulate that chunks are the actual content of phenomenal experience, Perruchet and Vinter [48, 49] outlined a view of the human mind in which consciousness is thought of as self- organized. In this model, the optimal coding of the incoming information occurs as a natural by-product of the evolution of conscious percepts and representations, under the action of simple associative learning and memory processes. Recent evolution of research on both IL, initially aimed at studying rule abstraction in complex situations, and SL, initially focused on word segmentation, suggests that the two lines of research explore the same domain-general incidental learning processes. Bringing together these two domains of research, however, reveals a divergence between the interpretation favored in IL, which focuses on the formation of chunks, and the interpretation favored in SL, which relies on statistical computations. One possibility is that chunks are inferred from the results of (unconscious) statistical computations. Another possibility is that (perhaps conscious) chunks are formed from the outset and then evolve as a result of basic associative learning principles. It is clear that there are many challenges for future research in these two areas (see Box 5). This work was supported by grants from the Centre National de la Recherche Scientifique (CNRS, UMR 5022 and FRE 2987), from the Universit  ́ de Bourgogne, from the R  ́gion de Bourgogne (AAFE), and from the Universit  ́ Paris V. The authors thank Stephanie Chambaron, Suzanne Filipic, Bob French, Barbara Tillmann, and the anonymous reviewers of a first draft for their help at various stages of elaboration.
